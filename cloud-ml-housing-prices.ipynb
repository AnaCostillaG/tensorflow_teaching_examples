{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Distributed Machine Learning using Tensorflow + Cloud ML Engine\n",
    "\n",
    "A barebones example showing the simplest path from raw data to distributed cloud training with the fewest lines of code\n",
    "\n",
    "**This notebook is intended to be run on Google Cloud Datalab**: https://cloud.google.com/datalab/docs/quickstarts\n",
    "\n",
    "Datalab will have the required libraries installed by default for this code to work. If you choose to run this code outside of Datalab you may run in to version and dependency issues which you will need to resolve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Steps\n",
    "\n",
    "1. Load raw data\n",
    "\n",
    "2. Define feature columns\n",
    "\n",
    "3. Define Estimator\n",
    "\n",
    "4. Define input function\n",
    "\n",
    "5. Define Experiment\n",
    "\n",
    "6. Package Code\n",
    "\n",
    "7. Run!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1) Load Raw Data\n",
    "\n",
    "The boston housing prices dataset. It is a tab separated text file hosted in a public GCS bucket.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#downlad data from GCS and store as pandas dataframe \n",
    "df = pd.read_csv(\n",
    "  filepath_or_buffer='https://storage.googleapis.com/vijay-public/boston_housing/housing.data.txt',\n",
    "  delim_whitespace=True,\n",
    "  names=[\"CRIM\",\"ZN\",\"INDUS\",\"CHAS\",\"NOX\",\"RM\",\"AGE\",\"DIS\",\"RAD\",\"TAX\",\"PTRATIO\",\"B\",\"LSTAT\",\"MEDV\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296.0   \n",
       "1  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242.0   \n",
       "2  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242.0   \n",
       "3  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222.0   \n",
       "4  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  MEDV  \n",
       "0     15.3  396.90   4.98  24.0  \n",
       "1     17.8  396.90   9.14  21.6  \n",
       "2     17.8  392.83   4.03  34.7  \n",
       "3     18.7  394.63   2.94  33.4  \n",
       "4     18.7  396.90   5.33  36.2  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2) Define feature columns\n",
    "\n",
    "Feature columns are your Estimator's data \"interface.\" They tell the estimator in what format they should expect data and how to interpret it (is it one-hot? sparse? dense? categorical? conteninous?).  https://www.tensorflow.org/api_docs/python/tf/contrib/layers/feature_column\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "FEATURES = [\"CRIM\", \"ZN\", \"INDUS\", \"NOX\", \"RM\",\n",
    "            \"AGE\", \"DIS\", \"TAX\", \"PTRATIO\"]\n",
    "LABEL = \"MEDV\"\n",
    "\n",
    "feature_cols = [tf.contrib.layers.real_valued_column(k)\n",
    "                  for k in FEATURES] #list of Feature Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3) An Estimator is what actually implements your training, eval and prediction loops. Every estimator has the following methods:\n",
    "\n",
    "- fit() for training\n",
    "- eval() for evaluation\n",
    "- predict() for prediction\n",
    "- export_savedmodel() for writing model state to disk\n",
    "\n",
    "Tensorflow has several canned estimator that already implement these methods (DNNClassifier, LogisticClassifier etc..) or you can implement a custom estimator. For an example of implementing a custom estimator see here:https://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/blogs/timeseries/rnn_cloudmle.ipynb\n",
    "\n",
    "For simplicity we will use a canned estimator. To instantiate an estimator simply pass it what Feature Columns to expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def generate_estimator(output_dir):\n",
    "  return tf.contrib.learn.DNNRegressor(feature_columns=feature_cols,\n",
    "                                            hidden_units=[10, 10],\n",
    "                                            model_dir=output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 4) Define input function\n",
    "\n",
    "Now that you have an estimator and it konws what type of data to expect and how to intepret, you need to actually pass the data to it! This is the job of the input function. \n",
    "\n",
    "The input function return a (features, label) tuple\n",
    "- features: A dictionary. Each key is a feature column name and its value is the Tensor containing the data for that Feature\n",
    "- label: A Tensor containing the label column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def generate_input_fn(data_set):\n",
    "    def input_fn():\n",
    "      features = {k: tf.constant(data_set[k].values) for k in FEATURES}\n",
    "      labels = tf.constant(data_set[LABEL].values)\n",
    "      return features, labels\n",
    "    return input_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 5) Define Experiment\n",
    "\n",
    "An experiment is ultimatley what you pass to learn_runner.run() for Cloud ML training. It encapsulated the Estimator (which in turn encapsulates the feature column definitions) and the Input Function.\n",
    "\n",
    "Notice that to instantiate an Experiment you don't pass the Estimator or Input Functions directly. Instead you pass a function that *returns* an Estimator or Input Function. This is why we wrapped the Estimator and Input Function instantiations in generator functions during steps 3 and 4 respectively. It is also why we wrap the Experiment itself in a generator function, because a downstream function will expect it in this format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def generate_experiment_fn(output_dir):\n",
    "  return tf.contrib.learn.Experiment(\n",
    "    generate_estimator(output_dir),\n",
    "    train_input_fn=generate_input_fn(df), \n",
    "    eval_input_fn=generate_input_fn(df), #Normally you would pass a different eval set\n",
    "    train_steps=1000,\n",
    "    eval_steps=1000\n",
    "  )\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 6) Package Code\n",
    "\n",
    "You've now written all the tensoflow code you need!\n",
    "\n",
    "To make it compatible with Cloud ML Engine we'll combine the above tensorflow code into a single python file with two simple changes\n",
    "\n",
    "1. Add some boilerplate code to parse the command line arguments required for gcloud.\n",
    "2. Use the learn_runner.run() function to run the experiment\n",
    "\n",
    "We also add an \\__init__\\.py file to the foler. This is just python convention for identifying modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: trainer: File exists\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "mkdir trainer\n",
    "touch trainer/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting trainer/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile trainer/task.py\n",
    "\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.learn.python.learn import learn_runner\n",
    "\n",
    "df = pd.read_csv(\n",
    "  filepath_or_buffer='https://storage.googleapis.com/vijay-public/boston_housing/housing.data.txt',\n",
    "  delim_whitespace=True,\n",
    "  names=[\"CRIM\",\"ZN\",\"INDUS\",\"CHAS\",\"NOX\",\"RM\",\"AGE\",\"DIS\",\"RAD\",\"TAX\",\"PTRATIO\",\"B\",\"LSTAT\",\"MEDV\"])\n",
    "\n",
    "FEATURES = [\"CRIM\", \"ZN\", \"INDUS\", \"NOX\", \"RM\",\n",
    "            \"AGE\", \"DIS\", \"TAX\", \"PTRATIO\"]\n",
    "LABEL = \"MEDV\"\n",
    "\n",
    "feature_cols = [tf.contrib.layers.real_valued_column(k)\n",
    "                  for k in FEATURES] #list of Feature Columns\n",
    "\n",
    "def generate_estimator(output_dir):\n",
    "  return tf.contrib.learn.DNNRegressor(feature_columns=feature_cols,\n",
    "                                            hidden_units=[10, 10],\n",
    "                                            model_dir=output_dir)\n",
    "\n",
    "def generate_input_fn(data_set):\n",
    "    def input_fn():\n",
    "      features = {k: tf.constant(data_set[k].values) for k in FEATURES}\n",
    "      labels = tf.constant(data_set[LABEL].values)\n",
    "      return features, labels\n",
    "    return input_fn\n",
    "\n",
    "def generate_experiment_fn(output_dir):\n",
    "  return tf.contrib.learn.Experiment(\n",
    "    generate_estimator(output_dir),\n",
    "    train_input_fn=generate_input_fn(df),\n",
    "    eval_input_fn=generate_input_fn(df),\n",
    "    train_steps=1000,\n",
    "    eval_steps=1000\n",
    "  )\n",
    "\n",
    "######CLOUD ML ENGINE BOILERPLATE CODE BELOW######\n",
    "if __name__ == '__main__':\n",
    "  parser = argparse.ArgumentParser()\n",
    "  # Input Arguments\n",
    "  parser.add_argument(\n",
    "      '--output_dir',\n",
    "      help='GCS location to write checkpoints and export models',\n",
    "      required=True\n",
    "  )\n",
    "  parser.add_argument(\n",
    "        '--job-dir',\n",
    "        help='this model ignores this field, but it is required by gcloud',\n",
    "        default='junk'\n",
    "    )\n",
    "  args = parser.parse_args()\n",
    "  arguments = args.__dict__\n",
    "  output_dir = arguments.pop('output_dir')\n",
    "\n",
    "  learn_runner.run(generate_experiment_fn, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 7) Run using gcloud command line tool\n",
    "Now that our code is packaged we can invoke it using gcloud to run the training. \n",
    "\n",
    "Note: Since our dataset is so small and our model is simple the overhead of provisioning the cluster is longer than the actual training time. Accordingly you'll notice the single VM cloud training takes longer than the local training, and the distributed cloud training takes longer than single VM cloud. For larger datasets and more complex models this will reverse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run local\n",
    "It's a best practice to first run locally on a small dataset to check for errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Usage: gcloud [optional flags] <group | command>\n",
      "  group may be           alpha | app | auth | beta | components | compute |\n",
      "                         config | container | dataproc | deployment-manager |\n",
      "                         dns | iam | preview | projects | source | sql | topic\n",
      "  command may be         docker | feedback | help | info | init | version\n",
      "\n",
      "The *gcloud* CLI manages authentication, local configuration, developer\n",
      "workflow, and interactions with the Google Cloud Platform APIs.\n",
      "\n",
      "global flags:\n",
      "  --account=ACCOUNT      Google Cloud Platform user account to use for\n",
      "                         invocation.\n",
      "  --configuration=CONFIGURATION\n",
      "                         The configuration to use for this command invocation.\n",
      "  --flatten=[KEY,...]    Flatten _name_[] output resource slices in _KEY_ into\n",
      "                         separate records for each item in each slice.\n",
      "  --format=FORMAT        The format for printing command output resources.\n",
      "  --help                 Display detailed help.\n",
      "  --project=PROJECT_ID   Google Cloud Platform project ID to use for this\n",
      "                         invocation.\n",
      "  --quiet, -q            Disable all interactive prompts.\n",
      "  --verbosity=VERBOSITY; default=\"warning\"\n",
      "                         Override the default verbosity for this command.\n",
      "  -h                     Print a summary help and exit.\n",
      "  -v, --version          Print version information and exit. This flag is only\n",
      "                         available at the global level.\n",
      "\n",
      "other flags:\n",
      "  Run `gcloud --help`\n",
      "  for the full list of available flags for this command.\n",
      "\n",
      "command groups:\n",
      "  alpha                  *(ALPHA)* Alpha versions of gcloud commands.\n",
      "  app                    Manage your App Engine app.\n",
      "  auth                   Manage oauth2 credentials for the Google Cloud SDK.\n",
      "  beta                   *(BETA)* Beta versions of gcloud commands.\n",
      "  components             List, install, update, or remove Google Cloud SDK\n",
      "                         components.\n",
      "  compute                Read and manipulate Google Compute Engine resources.\n",
      "  config                 View and edit Cloud SDK properties.\n",
      "  container              Deploy and manage clusters of machines for running\n",
      "                         containers.\n",
      "  dataproc               Create and manage Google Cloud Dataproc clusters and\n",
      "                         jobs.\n",
      "  deployment-manager     Manage deployments of cloud resources.\n",
      "  dns                    Manage your Cloud DNS managed-zones and record-sets.\n",
      "  iam                    Manage IAM service accounts and keys.\n",
      "  preview                *(PREVIEW)* Preview versions of gcloud commands.\n",
      "  projects               Manage your projects.\n",
      "  source                 Cloud git repository commands.\n",
      "  sql                    Manage Cloud SQL databases.\n",
      "  topic                  gcloud supplementary help.\n",
      "\n",
      "commands:\n",
      "  docker                 Provides the docker CLI access to the Google Container\n",
      "                         Registry.\n",
      "  feedback               Provide feedback to the Google Cloud SDK team.\n",
      "  help                   Prints detailed help messages for the specified\n",
      "                         commands.\n",
      "  info                   Display information about the current gcloud\n",
      "                         environment.\n",
      "  init                   Initialize or reinitialize gcloud.\n",
      "  version                Print version information for Cloud SDK components.\n",
      "\n",
      "\n",
      "For more detailed information on this command and its flags, run:\n",
      "  gcloud --help\n",
      "\n",
      "ERROR: (gcloud) Invalid choice: 'ml-engine'.\n",
      "\n",
      "Valid choices are [alpha, app, auth, beta, components, compute, config, container, dataproc, deployment-manager, dns, docker, feedback, help, iam, info, init, meta, preview, projects, source, sql, topic, version].\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gcloud ml-engine local train \\\n",
    "   --module-name=trainer.task \\\n",
    "   --package-path=trainer \\\n",
    "   -- \\\n",
    "   --output_dir='./output'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Run on cloud (1 cloud ML unit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "First we specify which GCP project to use. Update the cell below to use your project name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gcloud config set project vijays-sandbox #CHANGE THIS TO YOUR PROJECT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Then we specify which GCS bucket to write to and a job name.\n",
    "Job names submitted to the ml engine must be project unique, so we append the system date/time. Update the cell below to point to a GCS bucket you own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jobId: housing_170326_233617\n",
      "state: QUEUED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Job [housing_170326_233617] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ml-engine jobs describe housing_170326_233617\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ml-engine jobs stream-logs housing_170326_233617\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "GCS_BUCKET=gs://vijays-sandbox-ml/housing #CHANGE THIS TO YOUR GCS BUCKET\n",
    "JOBNAME=housing_$(date -u +%y%m%d_%H%M%S)\n",
    "\n",
    "gcloud ml-engine jobs submit training $JOBNAME \\\n",
    "   --region=us-central1 \\\n",
    "   --module-name=trainer.task \\\n",
    "   --package-path=./trainer \\\n",
    "   --job-dir=$GCS_BUCKET/$JOBNAME/ \\\n",
    "   -- \\\n",
    "   --output_dir=$GCS_BUCKET/$JOBNAME/output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Run on cloud (10 cloud ML units)\n",
    "Because we are using the TF Experiments interface, distributed computing just works! The only change we need to make to run in a distributed fashion is to add the [--scale-tier](https://cloud.google.com/ml/pricing#ml_training_units_by_scale_tier) argument. Cloud ML Engine then takes care of distributing the training across devices for you!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jobId: housing_170326_233648\n",
      "state: QUEUED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Job [housing_170326_233648] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ml-engine jobs describe housing_170326_233648\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ml-engine jobs stream-logs housing_170326_233648\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "GCS_BUCKET=gs://vijays-sandbox-ml/housing #CHANGE THIS TO YOUR GCS BUCKET\n",
    "JOBNAME=housing_$(date -u +%y%m%d_%H%M%S)\n",
    "\n",
    "gcloud ml-engine jobs submit training $JOBNAME \\\n",
    "   --region=us-central1 \\\n",
    "   --module-name=trainer.task \\\n",
    "   --package-path=./trainer \\\n",
    "   --job-dir=$GCS_BUCKET/$JOBNAME \\\n",
    "   --scale-tier=STANDARD_1 \\\n",
    "   -- \\\n",
    "   --output_dir=$GCS_BUCKET/$JOBNAME/output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Run on cloud GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Also works with GPUs!\n",
    "\n",
    "\"BASIC_GPU\" corresponds to one Tesla K80 at the time of this writing, hardware subject to change. It is charged as 3 Cloud ML units. Some types of trainings are better suited for GPU than others. Make sure it speeds up your training at least 3x to be cost effective!\n",
    "\n",
    "If you want to train across multiple GPUs you would use a [custom scale tier](https://cloud.google.com/ml/docs/concepts/training-overview#job_configuration_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jobId: housing_170326_233920\n",
      "state: QUEUED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Job [housing_170326_233920] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ml-engine jobs describe housing_170326_233920\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ml-engine jobs stream-logs housing_170326_233920\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "GCS_BUCKET=gs://vijays-sandbox-ml/housing #CHANGE THIS TO YOUR GCS BUCKET\n",
    "JOBNAME=housing_$(date -u +%y%m%d_%H%M%S)\n",
    "\n",
    "gcloud ml-engine jobs submit training $JOBNAME \\\n",
    "   --region=us-east1 \\\n",
    "   --module-name=trainer.task \\\n",
    "   --package-path=./trainer \\\n",
    "   --job-dir=$GCS_BUCKET/$JOBNAME \\\n",
    "   --scale-tier=BASIC_GPU \\\n",
    "   -- \\\n",
    "   --output_dir=$GCS_BUCKET/$JOBNAME/output"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:python2-dlnd]",
   "language": "python",
   "name": "conda-env-python2-dlnd-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
